ğŸ“Š Retail Sales Analytics with Databricks & PySpark

An end-to-end big data analytics project demonstrating scalable data processing and business intelligence using Databricks Community Edition and Apache Spark (PySpark). This project ingests a real-world retail dataset from Kaggle, performs comprehensive cleaning and transformation, and delivers actionable business insights through advanced analytics and visualizations.

ğŸ”— Live Databricks Notebook
[Open the Interactive Notebook Here](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3675438403174862/3900883391960238/2977761025472207/latest.html)
Note: This public link is valid for approximately 6 months from its creation.
________________________________________________
ğŸ“‹ Project Overview
This project implements a full data analytics pipeline:

Data Ingestion: Automated download of a Kaggle dataset directly into Databricks.

Data Processing: Cleaning, transformation, and feature engineering using PySpark.

Advanced Analytics: Customer segmentation, trend analysis, and profitability studies.

Visualization & Reporting: Generation of key business metrics and professional charts.
________________________________________________
ğŸ› ï¸ Technology Stack
Category	Tools & Libraries
Cloud Platform	Databricks Community Edition
Processing Engine	Apache Spark 3.x (PySpark API)
Core Libraries	pyspark.sql, pyspark.sql.functions
Data Visualization	Matplotlib, Seaborn
Data Management	Pandas (for charting), KaggleHub API
Storage Format	Parquet (for optimized results)
________________________________________________
ğŸ“ Project Folder Structure (in Databricks DBFS)

/FileStore/

â”œâ”€â”€ kaggle/                          # Raw dataset downloaded from Kaggle

â”‚   â””â”€â”€ superstore/train.csv

â”œâ”€â”€ parquet/                         # Optimized analysis outputs

â”‚   â”œâ”€â”€ rfm_analysis.parquet

â”‚   â”œâ”€â”€ quarterly_sales.parquet

â”‚   â””â”€â”€ customer_lifetime_value.parquet

â”œâ”€â”€ results/                         # Final reports and results

â”‚   â””â”€â”€ retail_analytics_report.md

â”œâ”€â”€ charts/                          # Generated visualization PNG files

â”‚   â”œâ”€â”€ quarterly_sales_trend.png

â”‚   â”œâ”€â”€ category_performance.png

â”‚   â”œâ”€â”€ rfm_segmentation.png

â”‚   â”œâ”€â”€ monthly_trend_line.png

â”‚   â”œâ”€â”€ top_customers.png

â”‚   â””â”€â”€ shipping_performance.png

â””â”€â”€ portfolio/                       # Assets for GitHub portfolio

    â””â”€â”€ project_summary.md
________________________________________________

ğŸ“‚ Dataset Information
Source: Kaggle: Sales Forecasting Dataset by rohitsahoo

File Used: train.csv

Records: 9,800 sales transactions

Time Period: January 2015 to December 2018 (4 years)

Key Columns: Order ID, Date, Customer, Product Category, Sales, Region, Ship Mode.
________________________________________________

ğŸ“ˆ Key Business Metrics Calculated
The analysis computed the following core metrics:

Total Revenue: Sum of all Sales.

Average Order Value (AOV): Mean of Sales.

Customer Base: Count of distinct Customer ID.

Order Volume: Count of distinct Order ID.

Shipping Efficiency: Average shipping_days (derived from Ship Date - Order Date).

Year-over-Year (YoY) Growth: Percentage change in quarterly sales.
________________________________________________

âš™ï¸ PySpark & Spark SQL Functions Used
The notebook extensively uses PySpark's DataFrame API and functions for transformation and aggregation:

ğŸ”§ Data Transformation & Cleaning

to_date(): Convert string columns to DateType.

withColumn() / withColumnRenamed(): Create new or rename columns.

cast(): Change column data types (e.g., String to Double).

when() / otherwise(): Implement conditional logic (similar to SQL CASE WHEN).

regexp_replace(): Clean string values (e.g., remove currency symbols from sales).

year(), month(), quarter(): Extract date parts for trend analysis.

datediff(): Calculate the difference in days between two dates (for shipping time).
________________________________________________

ğŸ“Š Aggregation & Analysis
groupBy(): Group data for aggregate calculations.

agg(): Perform multiple aggregations (sum, count, average).

sum() / avg() / count() / countDistinct(): Basic aggregate functions.

min() / max(): Find minimum and maximum values.

orderBy(): Sort results.

describe(): Generate summary statistics.

________________________________________________
ğŸ¯ Advanced Analytics (Window Functions & SQL)
Window() + orderBy(): Define window specifications.

ntile(): Assign percentile ranks (used for RFM scoring).

lag(): Access data from a previous row in the same result set (used for YoY growth calculation).

createOrReplaceTempView(): Register a DataFrame as a temporary SQL view.

Spark SQL Queries: Direct SQL syntax (e.g., %sql SELECT ...) was used for complex joins and analysis.
________________________________________________

ğŸ“Š Generated Visualizations (PNG Files)
The project created six key charts, saved as high-resolution PNG files:
1. Quarterly Sales Performance (2015-2018)
https://charts/quarterly_sales_trend.png
Bar chart showing sales performance across all quarters (2015-2018).

2. Product Category Distribution
https://charts/category_performance.png
Pie chart displaying revenue distribution across Furniture, Office Supplies, and Technology.

3. Customer RFM Segmentation
https://charts/rfm_segmentation.png
Bar chart visualizing the 8 customer segments (Champions, Loyal, At Risk, etc.) from the RFM analysis.

4. Monthly Sales Trend
https://charts/monthly_trend_line.png
Line chart with a trend line illustrating sales over the last 24 months.

5. Top Customers by Lifetime Value
https://charts/top_customers.png
Horizontal bar chart ranking the top 10 customers by Lifetime Value (LTV).

 6. Shipping Mode Performance
https://charts/shipping_performance.png
Dual chart comparing shipping mode volume (pie) and average speed (bar).
________________________________________________

ğŸ“ Key Insights Summary
Sales Trend: Strong growth observed, with Q4 2018 being the peak sales quarter.

Top Category: Technology generated the highest revenue, with Phones as the leading sub-category.

Customer Segments: RFM analysis identified 195 "Champion" customers who are the most valuable.

Shipping: "Standard Class" is the most common but slowest mode; "Same Day" shipping is most efficient in the West region.

Geography: California (West region) is the top-performing state by sales volume.

________________________________________________

ğŸ“Š FINAL PROJECT SUMMARY & KEY METRICS
============================================================
RETAIL SALES ANALYTICS - SUPERSTORE DATASET
============================================================

ğŸ“ˆ KEY PERFORMANCE METRICS:
----------------------------------------
ğŸ’° Total Revenue:          $2,237,133.16
ğŸ“¦ Total Orders:           4,922
ğŸ‘¥ Unique Customers:       793
ğŸ“Š Average Order Value:    $235.29
ğŸ“… Analysis Period:        2015-01-03 to 2018-12-30
ğŸ“¦ Products Analyzed:      1,861
ğŸšš Avg Shipping Days:      3.96 days

ğŸ” TOP BUSINESS FINDINGS:
----------------------------------------
1. ğŸ“ˆ Q4 2018 was the best quarter with $274,516 in sales
2. ğŸ† Technology category generated highest revenue
3. ğŸ‘‘ 195 'Champion' customers drive 40% of total revenue
4. ğŸš€ Sales grew 18.86% YoY in latest quarter
5. ğŸ“¦ Standard Class shipping is most used but slowest (5+ days)
6. ğŸ¤ Office Supplies + Technology is most common product bundle
7. ğŸ† Top customer 'Sean Miller' spent $25,043 across 5 orders
8. ğŸŒ West region has fastest 'Same Day' shipping efficiency

________________________________________________
âœ¨ Skills Demonstrated
This project showcases practical skills in:

Building production-style data pipelines on Databricks.

Performing large-scale data wrangling with PySpark DataFrames.

Implementing advanced analytics (RFM, CLV, Time Series).

Translating data insights into clear business recommendations.

Creating automated reports and professional visualizations.

This project was developed as a portfolio piece to demonstrate end-to-end competency in Big Data analytics.
Analyst: Buvi | Tools: Databricks, PySpark, Python

